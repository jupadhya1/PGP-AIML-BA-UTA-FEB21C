# -*- coding: utf-8 -*-
"""TensorFlow with GPU

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/gpu.ipynb

# Tensorflow with GPU

This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.

## Enabling and testing the GPU

First, you'll need to enable GPUs for the notebook:

- Navigate to Editâ†’Notebook Settings
- select GPU from the Hardware Accelerator drop-down

Next, we'll confirm that we can connect to the GPU with tensorflow:
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""## Observe TensorFlow speedup on GPU relative to CPU

This example constructs a typical convolutional neural network layer over a
random image and manually places the resulting ops on either the CPU or the GPU
to compare execution speed.
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    random_image_cpu = tf.random.normal((100, 100, 100, 3))
    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)
    return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    random_image_gpu = tf.random.normal((100, 100, 100, 3))
    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)
    return tf.math.reduce_sum(net_gpu)
  
# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
cpu()
gpu()

# Run the op several times.
print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

# Evalautaion of Linear Regression Model

# Cross Validation Regression MAE
import pandas
from sklearn import model_selection
from sklearn.linear_model import LinearRegression
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data"
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)
array = dataframe.values
X = array[:,0:13]
Y = array[:,13]
kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)
model = LinearRegression()
scoring = 'neg_mean_absolute_error'
results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print("MAE: %.3f (%.3f)" % (results.mean(), results.std()))

# Cross Validation Regression MSE
import pandas
from sklearn import model_selection
from sklearn.linear_model import LinearRegression
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data"
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)
array = dataframe.values
X = array[:,0:13]
Y = array[:,13]
kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)
model = LinearRegression()
scoring = 'neg_mean_squared_error'
results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print("MSE: %.3f (%.3f)" % (results.mean(), results.std()))

# Cross Validation Regression R^2
import pandas
from sklearn import model_selection
from sklearn.linear_model import LinearRegression
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data"
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
dataframe = pandas.read_csv(url, delim_whitespace=True, names=names)
array = dataframe.values
X = array[:,0:13]
Y = array[:,13]
kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)
model = LinearRegression()
scoring = 'r2'
results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print("R^2: %.3f (%.3f)" % (results.mean(), results.std()))

from sklearn.metrics import mean_absolute_error

y_true = [3,-.05,2,7]
y_pred = [2.5, 0.0, 2, 8]

mean_absolute_error(y_true,y_pred)

from sklearn.metrics import mean_squared_error

mean_squared_error(y_true,y_pred)

from sklearn.metrics import r2_score

r2_score(y_true,y_pred)

# Assumption of Linear Regression Model

## Importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

## Load dataset
df = pd.read_csv('/content/student_score.csv')
df.plot(x='Hours',y='Scores',style='o')

## Plot the graph
plt.title("Hours vs Percentage")    ## For linear regression assumptions
plt.xlabel("Hours studied")
plt.ylabel("Percentage Score")
plt.show()

## For checking the distrbution

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import seaborn as sns
from scipy.stats import norm

## input is the number of hours devoted
X = df.iloc[:,:-1].values
## output is percentage scored
y = df.iloc[:,1].values
## splitting the dataset
X_train,X_test,y_train,y_test = train_test_split(X, y,
test_size=0.3, random_state=21)

## training model
model = LinearRegression()
model.fit(X_train,y_train)
## making predictions
y_pred = model.predict(X_test)
residual = y_test-y_pred
sns.distplot(residual , fit=norm)
plt.title("Normal probability plot of residuals")
plt.xlabel("residuals")
plt.ylabel("frequency")

## For checking the distrbution with Q-Q plots
import statsmodels.api as sm
import pylab as py
# Randomly generating data points
data = np.random.normal(0,1,50)
sm.qqplot(data, line ='s')
py.title("Residuals distribution using Q-Q plots")
py.show()

K-S (0-1) # 0.05

## Creating heatmap 

import seaborn as sns
# visualizing the relation between student's score and marks scored
sns.heatmap(df.corr())

## importing statsmodels
import numpy as np
from statsmodels.stats.stattools import durbin_watson

a = np.array([1, 2, 3,6,7,8])
## using statsmodels.durbin_watson()
d = durbin_watson(a)  
print(d)

DW = 2 (NO AutoCorr)
DW 0 and 2 (+ve Corr)
DW 2 and 4 (-ve Corr)

# Another Case Study Advertisement Delima

# Commented out IPython magic to ensure Python compatibility.
# imports
import pandas as pd
import seaborn as sns
import statsmodels.formula.api as smf
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.model_selection import train_test_split
import numpy as np

# allow plots to appear directly in the notebook
# %matplotlib inline

data = pd.read_csv('/content/Advertising.csv', index_col=0)
data.head()

data.isnull().values.any()

df.isnull().sum()

data.head()

sns.pairplot(data,x_vars = ["TV","Radio","Newspaper"],y_vars = "Sales",kind="reg")

data.hist(bins=20)

sns.lmplot('TV','Sales',data=data)
sns.lmplot('Radio','Sales',data=data)
sns.lmplot('Newspaper','Sales',data=data)

# Assumption (Test of Normality)

sns.distplot(data.Sales,bins=10, color="blue",hist=True)

num_cols = data[["TV","Radio","Newspaper"]]
a = data.corr()['Sales'].sort_values(ascending=False)
print(a)

corrmap = data.corr()
f, ax = plt.subplots(figsize=(12,9))
sns.heatmap(corrmap, vmin = 0, vmax = 1, square=True, cmap="BuPu_r",ax=ax)
plt.show()

X = data.drop('Sales',axis=1)
y = data[['Sales']]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state = 33)

model = [('LinearRegression',LinearRegression())]

lin_model = smf.ols(formula="Sales~TV+Radio+Newspaper",data=data).fit()

print(lin_model.params,"\n")
print(lin_model.summary())

lin_model = smf.ols(formula="Sales~TV+Radio",data=data).fit()

print(lin_model.params,"\n")
print(lin_model.summary())

data.shape

sns.pairplot(data, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=7, aspect=0.7)

### STATSMODELS ###

# create a fitted model
lm1 = smf.ols(formula='Sales ~ TV', data=data).fit()

# print the coefficients
lm1.params

### SCIKIT-LEARN ###

# create X and y
feature_cols = ['Radio']
X = data[feature_cols]
y = data.Sales

# instantiate and fit
lm2 = LinearRegression()
lm2.fit(X, y)

# print the coefficients
print(lm2.intercept_)
print(lm2.coef_)

# manually calculate the prediction
7.032594 + 0.047537*50

### STATSMODELS ###

# you have to create a DataFrame since the Statsmodels formula interface expects it
X_new = pd.DataFrame({'TV': [50]})

# predict for a new observation
lm1.predict(X_new)

### SCIKIT-LEARN ###

# predict for a new observation
lm2.predict(50)

sns.pairplot(data, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=7, aspect=0.7, kind='reg')

### STATSMODELS ###

# print the confidence intervals for the model coefficients
lm1.conf_int()

### STATSMODELS ###

# print the p-values for the model coefficients
lm1.pvalues

### STATSMODELS ###

# print the R-squared value for the model
lm1.rsquared

### SCIKIT-LEARN ###

# print the R-squared value for the model
lm2.score(X, y)

### STATSMODELS ###

# create a fitted model with all three features
lm1 = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()

# print the coefficients
lm1.params

### SCIKIT-LEARN ###

# create X and y
feature_cols = ['TV', 'Radio']
X = data[feature_cols]
y = data.Sales

# instantiate and fit
lm2 = LinearRegression()
lm2.fit(X, y)

# print the coefficients
print(lm2.intercept_)
print(lm2.coef_)

# pair the feature names with the coefficients
list(zip(feature_cols, lm2.coef_))

### STATSMODELS ###

# print a summary of the fitted model
lm1.summary()

### STATSMODELS ###

# only include TV and Radio in the model

# instantiate and fit model
lm1 = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()

# calculate r-square 
lm1.rsquared

# add Newspaper to the model (which we believe has no association with Sales)
lm1 = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()
lm1.rsquared

# define true and predicted response values
y_true = [100, 50, 30, 20]
y_pred = [90, 50, 50, 30]

# calculate MAE, MSE, RMSE
print(metrics.mean_absolute_error(y_true, y_pred))
print(metrics.mean_squared_error(y_true, y_pred))
print(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))

# include Newspaper
X = data[['TV', 'Radio', 'Newspaper']]
y = data.Sales

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Instantiate model
lm2 = LinearRegression()

# Fit Model
lm2.fit(X_train, y_train)

# Predict
y_pred = lm2.predict(X_test)

# RMSE
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# exclude Newspaper
X = data[['TV', 'Radio']]
y = data.Sales

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Instantiate model
lm2 = LinearRegression()

# Fit model
lm2.fit(X_train, y_train)

# Predict
y_pred = lm2.predict(X_test)

# RMSE
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# set a seed for reproducibility
np.random.seed(12345)

# create a Series of booleans in which roughly half are True
nums = np.random.rand(len(data))
mask_large = nums > 0.5

# initially set Size to small, then change roughly half to be large
data['Size'] = 'small'

# Series.loc is a purely label-location based indexer for selection by label
data.loc[mask_large, 'Size'] = 'large'
data.head()

# create a new Series called Size_large
data['Size_large'] = data.Size.map({'small':0, 'large':1})
data.head()

# create X and y
feature_cols = ['TV', 'Radio', 'Newspaper', 'Size_large']
X = data[feature_cols]
y = data.Sales

# instantiate
lm2 = LinearRegression()
# fit
lm2.fit(X, y)

# print coefficients
list(zip(feature_cols, lm2.coef_))

# set a seed for reproducibility
np.random.seed(123456)

nums = np.random.rand(len(data))
mask_suburban = (nums > 0.33) & (nums < 0.66)
mask_urban = nums > 0.66
data['Area'] = 'rural'
# Series.loc is a purely label-location based indexer for selection by label
data.loc[mask_suburban, 'Area'] = 'suburban'
data.loc[mask_urban, 'Area'] = 'urban'
data.head()

# create three dummy variables using get_dummies
pd.get_dummies(data.Area, prefix='Area').head()

area_dummies = pd.get_dummies(data.Area, prefix='Area').iloc[:, 1:]
area_dummies.head()

data = pd.concat([data, area_dummies], axis=1)
data.head()

# create X and y
feature_cols = ['TV', 'Radio', 'Newspaper', 'Size_large', 'Area_suburban', 'Area_urban']
X = data[feature_cols]
y = data.Sales

# instantiate and fit
lm2 = LinearRegression()
lm2.fit(X, y)

# print the coefficients
list(zip(feature_cols, lm2.coef_))

# Second Method

# Creating DataFrame out of Advertising.csv
df = pd.read_csv("Advertising.csv")
df.drop("Unnamed: 0", axis=1,inplace=True)
 
# Separating Independent and dependent variables
X=df.drop(['Sales'],axis=1)
Y=df.Sales
 
# Fit Linear Regression
lr = LinearRegression()
model=lr.fit(X,Y)
y_pred1 = model.predict(X)
print("R-squared: {0}".format(metrics.r2_score(Y,y_pred1)))

plt.scatter(Y, (y_pred1))
plt.xlabel("Fitted values")
plt.ylabel("Residuals")

sns.pairplot(df)

from sklearn.preprocessing import PolynomialFeatures 
   
poly = PolynomialFeatures(degree = 2) 
X_poly = poly.fit_transform(X) 
   
poly.fit(X_poly, Y) 
X_poly = sm.add_constant(X_poly)
results = sm.OLS(Y,X_poly).fit()
 
print(results.summary())

plt.subplots(figsize=(10,5))
plt.subplot(1,2,1)
plt.title("Before")
plt.scatter(y_pred1, (Y-ypred1))
plt.xlabel("Fitted values")
plt.ylabel("Residuals")
 
plt.subplot(1,2,2)
plt.title("After")
plt.scatter(ypred2, (Y-ypred2))
plt.xlabel("Fitted values")
plt.ylabel("Residuals")